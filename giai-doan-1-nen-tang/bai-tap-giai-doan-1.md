# B√ÄI T·∫¨P GIAI ƒêO·∫†N 1: N·ªÄN T·∫¢NG

**T·ªïng th·ªùi gian**: 4-6 gi·ªù
**S·ªë b√†i t·∫≠p**: 15 exercises + 40 c√¢u tr·∫Øc nghi·ªám

---

## H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG

### C√°ch l√†m b√†i t·∫≠p hi·ªáu qu·∫£:

1. **T·ª± l√†m tr∆∞·ªõc**: Kh√¥ng xem ƒë√°p √°n ngay
2. **Vi·∫øt ra gi·∫•y**: Gi√∫p ghi nh·ªõ t·ªët h∆°n
3. **Check ƒë√°p √°n**: Sau khi ho√†n th√†nh
4. **Hi·ªÉu t·∫°i sao**: N·∫øu sai, hi·ªÉu t·∫°i sao sai
5. **L√†m l·∫°i**: C√°c b√†i sai sau 2-3 ng√†y

### Ti√™u ch√≠ ƒë·∫°t:
- **Exercises**: ƒê·∫°t ‚â•12/15 b√†i (80%)
- **Multiple Choice**: ƒê·∫°t ‚â•32/40 c√¢u (80%)
- **Overall**: ‚â•80% ‚Üí Pass, ti·∫øn sang Giai ƒëo·∫°n 2

---

## PART A: PRACTICAL EXERCISES

### Exercise 1: Test Objectives

**Scenario**: B·∫°n l√† tester cho m·ªôt mobile banking app m·ªõi.

**C√¢u h·ªèi**: Li·ªát k√™ 5 test objectives ch√≠nh cho project n√†y v√† gi·∫£i th√≠ch t·∫°i sao m·ªói objective quan tr·ªçng.

**Format tr·∫£ l·ªùi**:
```
Objective 1: [T√™n objective]
T·∫°i sao quan tr·ªçng: [Gi·∫£i th√≠ch]

[Repeat cho 5 objectives]
```

<details>
<summary><b>ƒê√ÅP √ÅN</b> (Click ƒë·ªÉ xem)</summary>

```
Objective 1: Find critical security defects
T·∫°i sao: Banking app handle sensitive financial data.
Security vulnerabilities c√≥ th·ªÉ lead to data breach,
financial loss, v√† loss of customer trust.

Objective 2: Verify functional requirements met
T·∫°i sao: App ph·∫£i support core banking functions
(transfer, bill payment, balance check). N·∫øu
functions kh√¥ng work ƒë√∫ng, users kh√¥ng th·ªÉ s·ª≠ d·ª•ng.

Objective 3: Ensure performance meets requirements
T·∫°i sao: Users expect fast response (< 2s). Slow app
d·∫´n ƒë·∫øn poor user experience v√† app abandonment.

Objective 4: Reduce risk of production failures
T·∫°i sao: Production failures trong banking app
= service disruption, customer complaints, regulatory
issues, v√† reputation damage.

Objective 5: Build confidence for go-live
T·∫°i sao: Stakeholders c·∫ßn confidence r·∫±ng app
stable v√† ready for thousands of users tr∆∞·ªõc khi
launch to public.
```

**Scoring**:
- 5 objectives relevant: 2 points
- Clear explanations: 3 points
- Total: 5 points
</details>

---

### Exercise 2: Testing vs Debugging

**Scenario**: Developer n√≥i v·ªõi b·∫°n: "I already tested my code, no bugs!"

**C√¢u h·ªèi**:
a) Gi·∫£i th√≠ch t·∫°i sao developer testing code c·ªßa h·ªç kh√°c v·ªõi independent testing
b) Li·ªát k√™ 3 l√Ω do t·∫°i sao v·∫´n c·∫ßn independent testing ngay c·∫£ khi developer ƒë√£ test

<details>
<summary><b>ƒê√ÅP √ÅN</b></summary>

```
a) Developer testing own code kh√°c Independent testing:

DEVELOPER TESTING:
- C√≥ bias (bi·∫øt code works th·∫ø n√†o)
- Focus v√†o implementation (white-box)
- C√≥ th·ªÉ assume scenarios won't happen
- Debugging mindset (fix n·∫øu th·∫•y l·ªói)
- Conflict of interest (own code)

INDEPENDENT TESTING:
- Objective (no bias)
- Focus v√†o requirements (black-box)
- No assumptions, test everything
- Testing mindset (find defects)
- No emotional attachment

b) 3 l√Ω do c·∫ßn independent testing:

1. DIFFERENT PERSPECTIVE:
   Developers test "does code work correctly?"
   Testers test "does it meet user needs?"
   ‚Üí Find different types of defects

2. OBJECTIVITY:
   Developers unconsciously skip scenarios h·ªç
   "know" won't be issue. Testers test everything
   ‚Üí Find more edge cases

3. SPECIALIZATION:
   Testers c√≥ testing expertise (test design
   techniques, test strategies) developers
   th∆∞·ªùng kh√¥ng c√≥
   ‚Üí More effective testing
```

**Scoring**:
- Part a: 2.5 points
- Part b: 2.5 points (0.83/l√Ω do)
- Total: 5 points
</details>

---

### Exercise 3: Error ‚Üí Defect ‚Üí Failure

**Scenario**: App Shopee c√≥ l·ªói trong t√≠nh nƒÉng √°p d·ª•ng voucher.

**C√¢u h·ªèi**: T·∫°o m·ªôt example ho√†n ch·ªânh th·ªÉ hi·ªán flow Error ‚Üí Defect ‚Üí Failure, bao g·ªìm:
- Error: H√†nh ƒë·ªông sai c·ªßa developer
- Defect: Bug c·ª• th·ªÉ trong code
- Execution: Test scenario
- Failure: K·∫øt qu·∫£ sai quan s√°t ƒë∆∞·ª£c

<details>
<summary><b>ƒê√ÅP √ÅN</b> (Example)</summary>

```
ERROR (Human mistake):
Developer hi·ªÉu nh·∫ßm requirement v·ªÅ voucher stacking.
Requirement: "Maximum 1 voucher per order"
Developer nghƒ©: "User c√≥ th·ªÉ apply multiple n·∫øu
total discount < 50%"

DEFECT (Bug in code):
Code:
```javascript
function applyVouchers(order, vouchers) {
  let totalDiscount = 0;
  for (let voucher of vouchers) {
    totalDiscount += voucher.amount;
    if (totalDiscount <= order.total * 0.5) {
      order.applyVoucher(voucher);  // ‚ùå DEFECT: No check for max 1
    }
  }
}
```

EXECUTION:
User ƒë·∫∑t order 1,000,000ƒë
User apply 2 vouchers:
- SAVE100K: -100,000ƒë
- FREESHIP: -30,000ƒë

FAILURE:
Expected: Only first voucher applied (SAVE100K)
          Final price: 900,000ƒë
          Error message: "Maximum 1 voucher per order"

Actual:   Both vouchers applied
          Final price: 870,000ƒë  ‚ùå FAILURE!
          No error message

Impact: Customer gets extra discount (loss for company)
```

**Scoring**:
- Error identified clearly: 1 point
- Defect v·ªõi code example: 2 points
- Execution scenario realistic: 1 point
- Failure v·ªõi expected vs actual: 1 point
- Total: 5 points
</details>

---

### Exercise 4: Root Cause Analysis

**Scenario**: App crash v·ªõi error "NullPointerException at PaymentService.java:45"

**C√¢u h·ªèi**: S·ª≠ d·ª•ng 5 Whys technique ƒë·ªÉ t√¨m root cause. Vi·∫øt 5 whys v√† identify root cause cu·ªëi c√πng.

<details>
<summary><b>ƒê√ÅP √ÅN</b> (Example)</summary>

```
SYMPTOM: App crash v·ªõi NullPointerException

WHY #1: T·∫°i sao crash?
‚îî‚îÄ V√¨ bi·∫øn 'user.billingAddress' l√† null ·ªü line 45

WHY #2: T·∫°i sao billingAddress null?
‚îî‚îÄ V√¨ user ch∆∞a set billing address (optional field)

WHY #3: T·∫°i sao code kh√¥ng check null?
‚îî‚îÄ V√¨ developer assume billing address always available

WHY #4: T·∫°i sao developer assume v·∫≠y?
‚îî‚îÄ V√¨ requirement kh√¥ng specify r√µ billing address
   l√† optional

WHY #5: T·∫°i sao requirement kh√¥ng r√µ r√†ng?
‚îî‚îÄ V√¨ kh√¥ng c√≥ requirement review process tr∆∞·ªõc
   khi development

ROOT CAUSE: No requirement review process

IMMEDIATE FIX: Add null check trong code
if (user.billingAddress != null) { ... }

LONG-TERM FIX:
1. Implement requirement review process
2. Review checklist ph·∫£i include edge cases
3. BA training v·ªÅ writing clear requirements
4. Developers ph·∫£i ask questions khi unclear
```

**Scoring**:
- 5 Whys logical: 2 points
- Root cause identified: 1 point
- Immediate fix: 1 point
- Long-term fix: 1 point
- Total: 5 points
</details>

---

### Exercise 5: Seven Testing Principles

**C√¢u h·ªèi**: Cho m·ªói scenario sau, identify which Testing Principle ƒë∆∞·ª£c demonstrate v√† explain why.

**Scenario A**: Tester ch·∫°y same 100 regression tests m·ªói sprint trong 6 th√°ng. Ban ƒë·∫ßu t√¨m ra 10 bugs/sprint, b√¢y gi·ªù ch·ªâ t√¨m 0-1 bug/sprint.

**Scenario B**: E-commerce website c√≥ 10 modules. Module "Payment" c√≥ 60% total bugs m·∫∑c d√π ch·ªâ chi·∫øm 10% code.

**Scenario C**: Medical device software passed t·∫•t c·∫£ 1000 test cases nh∆∞ng v·∫´n cause patient injury khi released.

**Scenario D**: Tester estimate r·∫±ng testing to√†n b·ªô combinations c·ªßa 20 input fields s·∫Ω take 50 years.

**Scenario E**: Bug found trong requirement phase, fixed trong 10 minutes b·∫±ng c√°ch clarify v·ªõi BA. Same bug n·∫øu found trong production s·∫Ω require 2 days hotfix.

<details>
<summary><b>ƒê√ÅP √ÅN</b></summary>

```
SCENARIO A: Principle #5 - Pesticide Paradox
Explanation: Same tests ch·∫°y l·∫°i nhi·ªÅu l·∫ßn kh√¥ng
c√≤n t√¨m ƒë∆∞·ª£c defects m·ªõi. Tests "wear out". C·∫ßn
add new tests cho new scenarios.

SCENARIO B: Principle #4 - Defects Cluster Together
Explanation: Bugs tend to concentrate trong m·ªôt s·ªë
modules nh·∫•t ƒë·ªãnh. Payment module l√† high-defect
area, c·∫ßn focus testing effort v√†o ƒë√≥.

SCENARIO C: Principle #7 - Absence-of-Defects Fallacy
Explanation: Zero bugs kh√¥ng = success. Software
c√≥ th·ªÉ bug-free nh∆∞ng kh√¥ng meet user needs ho·∫∑c
sai trong design. Need validation, not just verification.

SCENARIO D: Principle #2 - Exhaustive Testing is Impossible
Explanation: Kh√¥ng th·ªÉ test t·∫•t c·∫£ combinations.
Must prioritize d·ª±a tr√™n risk v√† use test design
techniques ƒë·ªÉ reduce s·ªë test cases c·∫ßn thi·∫øt.

SCENARIO E: Principle #3 - Early Testing Saves Time and Money
Explanation: Testing v√† finding bugs early trong
SDLC = cheaper v√† faster to fix. Cost to fix tƒÉng
exponentially theo time.
```

**Scoring**: 1 point per correct principle + explanation
- Total: 5 points
</details>

---

### Exercise 6: Test Activities

**Scenario**: Project m·ªõi - Develop mobile food delivery app (gi·ªëng GrabFood).

**C√¢u h·ªèi**: Cho m·ªói test activity, li·ªát k√™ 3 specific tasks b·∫°n s·∫Ω l√†m v√† 1 deliverable (output) ch√≠nh.

Activities to cover:
1. Test Planning
2. Test Analysis
3. Test Design
4. Test Implementation
5. Test Execution

<details>
<summary><b>ƒê√ÅP √ÅN</b> (Example)</summary>

```
1. TEST PLANNING
Tasks:
- Define test scope (in: ordering, out: restaurant management)
- Identify test approach (risk-based, focus payment & tracking)
- Estimate resources (2 testers, 6 weeks, Selenium + Appium)
Deliverable: Test Plan document

2. TEST ANALYSIS
Tasks:
- Review requirements ƒë·ªÉ understand features (menu, cart, checkout)
- Identify test conditions (TC-001: Add item to cart, TC-002: Remove item, etc.)
- Define test data needs (test restaurants, test users, test menu items)
Deliverable: Test Conditions list (50 conditions)

3. TEST DESIGN
Tasks:
- Design test cases from conditions (TC-001 ‚Üí Detailed steps)
- Apply test techniques (EP for quantity, BVA for delivery distance)
- Specify test data values (Restaurant A: 2km away, Item B: 50,000ƒë)
Deliverable: Test Cases document (200 test cases)

4. TEST IMPLEMENTATION
Tasks:
- Create automated scripts (Selenium cho web, Appium cho mobile)
- Setup test environment (Test server, Test database, Payment sandbox)
- Prepare test data (Load 100 test restaurants, 500 menu items)
Deliverable: Test Scripts + Test Environment ready

5. TEST EXECUTION
Tasks:
- Execute test cases manually and automated
- Compare actual vs expected results
- Log defects for failures (BUG-001: Cart total calculation wrong)
Deliverable: Test Execution Report + Defect Reports
```

**Scoring**:
- Each activity: 1 point (tasks + deliverable)
- Total: 5 points
</details>

---

### Exercise 7: Entry & Exit Criteria

**Scenario**: System testing phase cho banking app.

**C√¢u h·ªèi**:
a) Define 5 entry criteria (conditions ƒë·ªÉ B·∫ÆT ƒê·∫¶U system testing)
b) Define 5 exit criteria (conditions ƒë·ªÉ K·∫æT TH√öC system testing)

<details>
<summary><b>ƒê√ÅP √ÅN</b></summary>

```
a) ENTRY CRITERIA (to start system testing):

1. CODE COMPLETE:
   All planned features implemented and code-frozen

2. BUILD DEPLOYED:
   Application deployed to test environment successfully

3. TEST CASES READY:
   All system test cases reviewed and approved
   (minimum 300 test cases)

4. TEST ENVIRONMENT STABLE:
   Test server, database, v√† integration points ready
   v√† stable (99% uptime last week)

5. TEST DATA PREPARED:
   Test users created, test accounts funded,
   test transactions loaded

b) EXIT CRITERIA (to finish system testing):

1. TEST EXECUTION COMPLETE:
   100% planned test cases executed

2. PASS RATE ACHIEVED:
   ‚â•95% test cases passed

3. CRITICAL DEFECTS RESOLVED:
   Zero critical/high severity defects open

4. COVERAGE ACHIEVED:
   - 100% requirement coverage
   - ‚â•80% code coverage

5. STAKEHOLDER SIGN-OFF:
   Test completion report reviewed and approved
   by Test Manager v√† Project Manager
```

**Scoring**:
- Entry criteria (5 relevant): 2.5 points
- Exit criteria (5 relevant): 2.5 points
- Total: 5 points
</details>

---

### Exercise 8: Traceability

**Scenario**: B·∫°n c√≥ 3 requirements cho login feature:

- REQ-001: User can login with email and password
- REQ-002: User can reset password via email
- REQ-003: System locks account after 5 failed login attempts

**C√¢u h·ªèi**:
a) Create test cases cho m·ªói requirement (2 test cases/requirement)
b) Create a Traceability Matrix
c) Gi·∫£i th√≠ch 2 benefits c·ªßa traceability n√†y

<details>
<summary><b>ƒê√ÅP √ÅN</b></summary>

```
a) TEST CASES:

REQ-001: User can login with email and password
‚îú‚îÄ TC-001: Login with valid credentials ‚Üí Success
‚îî‚îÄ TC-002: Login with invalid password ‚Üí Error message

REQ-002: User can reset password via email
‚îú‚îÄ TC-003: Request reset with valid email ‚Üí Email sent
‚îî‚îÄ TC-004: Request reset with invalid email ‚Üí Error

REQ-003: System locks account after 5 failed attempts
‚îú‚îÄ TC-005: Fail login 4 times ‚Üí Account still active
‚îî‚îÄ TC-006: Fail login 5 times ‚Üí Account locked

b) TRACEABILITY MATRIX:

| Req ID  | Description           | Test Cases    | Status  | Defects |
|---------|-----------------------|---------------|---------|---------|
| REQ-001 | Login with creds     | TC-001,TC-002 | Passed  | -       |
| REQ-002 | Reset password       | TC-003,TC-004 | Passed  | -       |
| REQ-003 | Lock after 5 fails   | TC-005,TC-006 | Failed  | BUG-100 |

Coverage: 100% (3/3 requirements have test cases)

c) BENEFITS:

Benefit 1: COVERAGE VERIFICATION
Traceability matrix shows m·ªçi requirement ƒë·ªÅu
c√≥ test cases cover. N·∫øu REQ-004 added m√† kh√¥ng
c√≥ test case, d·ªÖ d√†ng spot missing coverage.

Benefit 2: IMPACT ANALYSIS
N·∫øu REQ-003 changes (e.g., lock after 3 attempts
instead of 5), traceability immediately shows
TC-005 v√† TC-006 c·∫ßn update. Kh√¥ng miss affected
test cases.
```

**Scoring**:
- Test cases relevant: 2 points
- Traceability matrix correct: 1.5 points
- Benefits explained: 1.5 points
- Total: 5 points
</details>

---

### Exercise 9: Test Roles

**Scenario**: Small startup (10 people) developing e-commerce platform. B·∫°n l√† ng∆∞·ªùi duy nh·∫•t responsible for testing.

**C√¢u h·ªèi**:
a) B·∫°n ph·∫£i cover c·∫£ Test Management v√† Testing activities. List ra daily/weekly tasks b·∫°n c·∫ßn l√†m.
b) Gi·∫£i th√≠ch challenges c·ªßa vi·ªác combine 2 roles n√†y
c) Recommend solution ƒë·ªÉ improve situation

<details>
<summary><b>ƒê√ÅP √ÅN</b></summary>

```
a) TASKS:

DAILY (Testing Role):
- Execute test cases (manual + automated)
- Log defects found
- Verify bug fixes
- Update test execution status
- Communicate v·ªõi dev team v·ªÅ bugs

WEEKLY (Test Management Role):
- Update test plan based on changes
- Report progress to management
  (% tests done, defects found, risks)
- Prioritize test cases based on risk
- Monitor schedule v√† adjust n·∫øu behind
- Coordinate v·ªõi PO v·ªÅ requirements

BOTH:
- Design new test cases for new features
- Maintain automation scripts
- Setup/maintain test environment

b) CHALLENGES:

Challenge 1: TIME CONFLICT
Management tasks (planning, reporting) take time
away from execution. Risk: Testing execution delayed.

Challenge 2: LACK OF OBJECTIVITY
Hard to objectively assess own test coverage v√†
quality. Might have blind spots.

Challenge 3: OVERLOAD
Too many responsibilities ‚Üí Potential burnout,
mistakes, ho·∫∑c rushed work.

Challenge 4: NO BACKUP
N·∫øu sick ho·∫∑c leave ‚Üí Testing completely stopped.
Single point of failure.

c) SOLUTIONS:

Short-term (Immediate):
- PRIORITIZE: Focus high-risk features, automate
  regression ƒë·ªÉ save time
- TIME MANAGEMENT: Block time for management tasks
  (e.g., Monday morning for planning/reporting)
- TOOLING: Use tools ƒë·ªÉ automate repetitive tasks
  (test management tool, CI/CD)

Long-term (As company grows):
- HIRE: Th√™m 1 tester ƒë·ªÉ share workload
- WHOLE TEAM APPROACH: Train developers to write
  unit tests, involve PO trong acceptance testing
- CLEAR ROLE SPLIT: N·∫øu hire Test Manager, b·∫°n
  focus on technical testing activities
```

**Scoring**:
- Tasks comprehensive: 1.5 points
- Challenges identified: 1.5 points
- Solutions practical: 2 points
- Total: 5 points
</details>

---

### Exercise 10: Test Automation - Automate or Not?

**C√¢u h·ªèi**: Cho m·ªói scenario sau, decide c√≥ n√™n automate kh√¥ng v√† explain why.

**Scenario A**: Login function - Will be used trong every test suite, run 100+ times

**Scenario B**: Exploratory testing for new UX design - Need human judgment on aesthetics

**Scenario C**: Data entry form v·ªõi 50 fields - Need to test all valid/invalid combinations (500+ test cases)

**Scenario D**: Performance testing - Simulate 10,000 concurrent users

**Scenario E**: One-time migration script - Run once ƒë·ªÉ migrate old data

<details>
<summary><b>ƒê√ÅP √ÅN</b></summary>

```
SCENARIO A: ‚úÖ AUTOMATE
Reasoning:
- High reuse (run 100+ times)
- Stable functionality (login rarely changes)
- Repetitive (same steps every time)
- ROI: High (saves many hours)
‚Üí Perfect candidate for automation

SCENARIO B: ‚ùå DO NOT AUTOMATE
Reasoning:
- Requires human judgment (aesthetics)
- Subjective evaluation (no clear pass/fail)
- Exploratory nature (undefined steps)
- One-time activity (design review)
‚Üí Manual exploratory testing better

SCENARIO C: ‚úÖ AUTOMATE
Reasoning:
- Large volume (500+ test cases)
- Data-driven testing (perfect for automation)
- Repetitive steps (just different data)
- Manual = Too time-consuming
‚Üí Automate v·ªõi data-driven framework

SCENARIO D: ‚úÖ AUTOMATE (Must automate)
Reasoning:
- IMPOSSIBLE manually (10,000 users)
- Performance tools required (JMeter, Gatling)
- Need to measure response time accurately
- Must simulate real load
‚Üí No alternative, must automate

SCENARIO E: ‚ùå DO NOT AUTOMATE
Reasoning:
- One-time activity (no reuse)
- Automation setup time > Manual execution time
- Script will never run again
- Better spend time on verification than automation
‚Üí Manual execution + thorough validation
```

**Scoring**:
- Each scenario: 1 point (correct decision + good reasoning)
- Total: 5 points
</details>

---

## PART B: MULTIPLE CHOICE QUESTIONS (40 c√¢u)

### Section 1: Fundamentals (10 c√¢u)

**Question 1**
Which of the following is a typical test objective?

A. Verifying that all code has been written
B. Preventing defects by evaluating work products
C. Ensuring developers write defect-free code
D. Proving that software is 100% defect-free

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Preventing defects through evaluation (static testing) l√† typical objective. A, C, D kh√¥ng realistic.
</details>

---

**Question 2**
Which statement correctly describes the difference between testing and debugging?

A. Testing finds defects; debugging analyzes and fixes them
B. Testing fixes defects; debugging finds them
C. Testing is done by developers; debugging by testers
D. Testing and debugging are the same activity

<details>
<summary>ƒê√°p √°n</summary>
<b>A</b> - Testing = Find defects (testers), Debugging = Analyze v√† fix (developers)
</details>

---

**Question 3**
What is the relationship between errors, defects, and failures?

A. Errors in requirements lead to defects in code, which may cause failures
B. Failures in testing lead to defects in code, which may cause errors
C. Defects in requirements lead to errors in code, which may cause failures
D. Errors and defects are the same; only failures are different

<details>
<summary>ƒê√°p √°n</summary>
<b>A</b> - Error (human mistake) ‚Üí Defect (in work product) ‚Üí Failure (when executed)
</details>

---

**Question 4**
Which testing principle states that the same tests lose effectiveness over time?

A. Testing shows presence of defects
B. Exhaustive testing is impossible
C. Defects cluster together
D. Pesticide paradox

<details>
<summary>ƒê√°p √°n</summary>
<b>D</b> - Pesticide paradox: Tests wear out, need to update regularly
</details>

---

**Question 5**
Why is it important to perform testing early in the SDLC?

A. It is required by all SDLC models
B. It reduces the cost of fixing defects
C. It eliminates the need for later testing
D. It guarantees defect-free software

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Early testing = Find defects early = Cheaper to fix (Principle #3)
</details>

---

**Question 6**
Which of the following is a consequence of the "defects cluster together" principle?

A. All modules will have equal numbers of defects
B. Testing should focus more on high-defect areas
C. Defects found late are more expensive to fix
D. Automated testing will find all defects

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Since defects cluster, focus testing effort on high-defect areas
</details>

---

**Question 7**
What is the main difference between Quality Assurance (QA) and Quality Control (QC)?

A. QA focuses on processes; QC focuses on products
B. QA focuses on products; QC focuses on processes
C. QA is performed by testers; QC by developers
D. QA and QC are the same activity

<details>
<summary>ƒê√°p √°n</summary>
<b>A</b> - QA = Process-oriented (prevent defects), QC = Product-oriented (detect defects)
</details>

---

**Question 8**
How does testing contribute to higher quality?

A. By finding and fixing all defects before release
B. By providing information to support decision-making
C. By replacing the need for code reviews
D. By guaranteeing zero defects in production

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Testing provides information (defect data, metrics) ƒë·ªÉ support decisions
</details>

---

**Question 9**
Which testing principle explains why 100% testing coverage does not guarantee a defect-free system?

A. Testing shows presence, not absence
B. Exhaustive testing is impossible
C. Early testing saves money
D. Absence-of-defects fallacy

<details>
<summary>ƒê√°p √°n</summary>
<b>A</b> - Cannot prove absence of defects, only presence (Principle #1)
</details>

---

**Question 10**
In which phase of the SDLC is it typically LEAST expensive to fix a defect?

A. Requirements
B. Design
C. Implementation
D. Production

<details>
<summary>ƒê√°p √°n</summary>
<b>A</b> - Defects found trong requirements phase cheapest to fix
</details>

---

### Section 2: Test Process (10 c√¢u)

**Question 11**
Which test activity involves analyzing the test basis to identify testable features?

A. Test planning
B. Test analysis
C. Test design
D. Test implementation

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Test analysis: Analyze test basis, identify test conditions
</details>

---

**Question 12**
What is the main purpose of test implementation?

A. To design test cases
B. To execute test procedures
C. To create testware needed for execution
D. To archive test results

<details>
<summary>ƒê√°p √°n</summary>
<b>C</b> - Test implementation: Create testware (scripts, data, environment)
</details>

---

**Question 13**
Which of the following is an output of test analysis?

A. Test plan
B. Test conditions
C. Test procedures
D. Defect reports

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Test analysis output: Test conditions list
</details>

---

**Question 14**
During test execution, what should a tester do when actual results differ from expected results?

A. Modify the test case to match actual results
B. Log a defect report
C. Ignore minor differences
D. Ask developer to change requirements

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Actual ‚â† Expected ‚Üí Log defect report
</details>

---

**Question 15**
What is the primary purpose of test monitoring and control?

A. To design test cases
B. To track progress and take corrective actions
C. To execute tests
D. To analyze requirements

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Monitoring: Track progress; Control: Take corrective actions
</details>

---

**Question 16**
Which activity is part of test completion?

A. Designing test cases
B. Executing test procedures
C. Archiving testware
D. Implementing test automation

<details>
<summary>ƒê√°p √°n</summary>
<b>C</b> - Test completion: Archive testware, lessons learned
</details>

---

**Question 17**
Why is traceability between requirements and test cases important?

A. It enables automation of tests
B. It helps verify coverage and perform impact analysis
C. It reduces the cost of testing
D. It eliminates the need for test planning

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Traceability: Verify coverage + Impact analysis khi changes
</details>

---

**Question 18**
Which of the following is testware?

A. Requirements document
B. Test plan
C. Source code
D. User manual

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Testware = Work products created trong test process (test plan, test cases, scripts, reports)
</details>

---

**Question 19**
What is an exit criterion?

A. Condition to start testing
B. Condition to finish testing
C. Reason for test failure
D. Type of test technique

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Exit criteria: Conditions ƒë·ªÉ finish/complete test activity
</details>

---

**Question 20**
Which role is primarily responsible for test planning and monitoring?

A. Developer
B. Tester
C. Test Manager
D. Business Analyst

<details>
<summary>ƒê√°p √°n</summary>
<b>C</b> - Test Manager: Planning, monitoring, controlling, reporting
</details>

---

### Section 3: Skills & Independence (10 c√¢u)

**Question 21**
Which skill is MOST important for a tester?

A. Programming ability
B. Attention to detail
C. Project management
D. System administration

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Attention to detail: Fundamental tester skill
</details>

---

**Question 22**
What is a benefit of independent testing?

A. Testers find different types of defects than developers
B. Independent testing is always cheaper
C. No communication needed v·ªõi development team
D. Eliminates need for developer testing

<details>
<summary>ƒê√°p √°n</summary>
<b>A</b> - Independent testing: Different perspective ‚Üí Different types of defects
</details>

---

**Question 23**
What is the "whole team approach"?

A. Everyone does testing, no dedicated testers needed
B. Everyone is responsible for quality
C. All testing is done by the team lead
D. Testing and development are completely separated

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Whole team approach: Shared responsibility for quality
</details>

---

**Question 24**
Which level of test independence is HIGHEST?

A. No independent testing
B. Testing by another team member
C. Testing by independent test team
D. Testing by independent organization

<details>
<summary>ƒê√°p √°n</summary>
<b>D</b> - Testing by independent organization (third-party) = Highest independence
</details>

---

**Question 25**
What is a drawback of high test independence?

A. Better objectivity
B. Different perspective
C. Communication gaps
D. More defects found

<details>
<summary>ƒê√°p √°n</summary>
<b>C</b> - Too much independence ‚Üí Communication gaps, "us vs them" mentality
</details>

---

**Question 26**
Which generic skill involves understanding the business domain of the test object?

A. Testing knowledge
B. Domain knowledge
C. Technical knowledge
D. Analytical thinking

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Domain knowledge: Understanding business context
</details>

---

**Question 27**
Why is good communication important for testers?

A. To write test automation scripts
B. To explain defects v√† collaborate with team
C. To design test cases
D. To execute tests faster

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Communication: Explain bugs, collaborate, report status
</details>

---

**Question 28**
What does "analytical thinking" mean for a tester?

A. Ability to write code
B. Ability to analyze problems v√† find patterns
C. Ability to manage projects
D. Ability to use test tools

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Analytical thinking: Analyze problems, find root causes, identify patterns
</details>

---

**Question 29**
In Agile, who is responsible for testing quality?

A. Only testers
B. Only developers
C. Only Product Owner
D. Whole team

<details>
<summary>ƒê√°p √°n</summary>
<b>D</b> - Agile whole team approach: Everyone responsible for quality
</details>

---

**Question 30**
What is a benefit of testers working closely with developers?

A. Testers can find defects faster
B. Developers don't need to write unit tests
C. Better understanding v√† collaboration
D. Testing phase can be skipped

<details>
<summary>ƒê√°p √°n</summary>
<b>C</b> - Close collaboration: Better understanding, faster feedback, shared responsibility
</details>

---

### Section 4: Tools & Automation (10 c√¢u)

**Question 31**
Which type of tool is used to manage test cases v√† track results?

A. Static analysis tool
B. Test management tool
C. Coverage measurement tool
D. Performance testing tool

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Test management tools: Jira, TestRail (manage cases, track results)
</details>

---

**Question 32**
What is a static analysis tool used for?

A. Executing automated tests
B. Analyzing code without execution
C. Measuring performance
D. Managing test cases

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Static analysis: Analyze code WITHOUT execution (SonarQube, ESLint)
</details>

---

**Question 33**
Which tool would be used for load testing?

A. Selenium
B. JMeter
C. SonarQube
D. TestRail

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - JMeter: Performance/load testing tool
</details>

---

**Question 34**
What is a benefit of test automation?

A. All testing can be automated
B. Automated tests can run frequently with minimal effort
C. Eliminates need for manual testing
D. Always cheaper than manual testing

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Automation benefit: Run frequently, minimal effort, fast feedback
</details>

---

**Question 35**
What is a risk of test automation?

A. Tests run too fast
B. Unrealistic expectations v·ªÅ automation coverage
C. Too much test coverage
D. Defects found too quickly

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Automation risk: Unrealistic expectations (automate everything, no maintenance, etc.)
</details>

---

**Question 36**
Which type of testing is BEST suited for automation?

A. Exploratory testing
B. Usability testing
C. Regression testing
D. Ad-hoc testing

<details>
<summary>ƒê√°p √°n</summary>
<b>C</b> - Regression testing: Repetitive, run nhi·ªÅu l·∫ßn ‚Üí Perfect cho automation
</details>

---

**Question 37**
Which tool would be used to measure code coverage?

A. Selenium
B. JaCoCo
C. JMeter
D. Jira

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - JaCoCo (Java) / Istanbul (JavaScript): Code coverage tools
</details>

---

**Question 38**
What is a disadvantage of over-reliance on test automation?

A. Tests run too frequently
B. May miss issues that require human observation
C. Too many defects found
D. Costs too little

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Over-reliance: Miss UX issues, usability problems c·∫ßn human judgment
</details>

---

**Question 39**
Which tool is used for API testing?

A. Selenium
B. Appium
C. Postman
D. JaCoCo

<details>
<summary>ƒê√°p √°n</summary>
<b>C</b> - Postman: API testing tool (REST APIs)
</details>

---

**Question 40**
What is the main purpose of CI/CD tools like Jenkins?

A. To write test cases
B. To automate build, test, and deployment
C. To manage test data
D. To execute manual tests

<details>
<summary>ƒê√°p √°n</summary>
<b>B</b> - Jenkins: CI/CD automation (build ‚Üí test ‚Üí deploy pipeline)
</details>

---

## SCORING & ASSESSMENT

### Part A: Practical Exercises
- Total: 50 points (10 exercises √ó 5 points)
- Pass: ‚â•40 points (80%)

### Part B: Multiple Choice
- Total: 40 points (40 questions √ó 1 point)
- Pass: ‚â•32 points (80%)

### Overall Assessment
- **Total**: 90 points
- **Pass**: ‚â•72 points (80%)
- **Good**: ‚â•77 points (85%)
- **Excellent**: ‚â•81 points (90%)

---

## NEXT STEPS

### If you scored ‚â•80%: üéâ
**CONGRATULATIONS!** B·∫°n ƒë√£ n·∫Øm v·ªØng Giai ƒëo·∫°n 1!

**Ready for**:
- üìö [Giai ƒëo·∫°n 2: Static Testing](../giai-doan-2-static-testing/module-2.1-static-testing-co-ban.md)

**Before moving on**:
- Review any questions b·∫°n missed
- Ensure hi·ªÉu r√µ t·∫•t c·∫£ concepts
- Rest 1-2 days tr∆∞·ªõc khi start Giai ƒëo·∫°n 2

---

### If you scored <80%: üìñ
**DON'T WORRY!** C·∫ßn √¥n l·∫°i m·ªôt s·ªë concepts.

**Action plan**:
1. Identify topics b·∫°n weak (check which questions missed)
2. Re-study relevant modules:
   - Module 1.1: Test objectives, Testing vs Debugging
   - Module 1.2: Errors/Defects/Failures, 7 Principles, QA vs Testing
   - Module 1.3: 7 Test Activities, Testware, Traceability
   - Module 1.4: Test Skills, Tools, Automation
3. L√†m l·∫°i exercises sau 2-3 ng√†y
4. Target: ‚â•80% tr∆∞·ªõc khi chuy·ªÉn sang Giai ƒëo·∫°n 2

---

**Good luck! You're doing great! üí™**
